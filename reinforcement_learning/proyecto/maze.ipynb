{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c73cb6",
   "metadata": {},
   "source": [
    "## 1. Introducion\n",
    "\n",
    "El Aprendizaje por Refuerzo (Reinforcement Learning) constituye una de las áreas más relevantes del campo del Aprendizaje Automático, al centrarse en la interacción entre un agente y su entorno mediante un proceso de toma de decisiones secuencial. A diferencia de los enfoques supervisados, donde el aprendizaje se basa en ejemplos con etiquetas, en  RL el conocimiento se adquiere a través de la experiencia directa: el agente ejecuta acciones, \n",
    "observa las consecuencias y ajusta su comportamiento para maximizar la recompensa acumulada. \n",
    "En este contexto, los entornos GridWorld proporcionan un escenario simplificado pero ilustrativo para estudiar los fundamentos del Reinforcement Learning. Estos entornos discretos permiten modelar conceptos de estado, acción y recompensa, así como analizar la evolución del aprendizaje mediante políticas y funciones de valor. El presente trabajo implementa un agente que interactúa dentro de un entorno laberinto de 8×7 celdas, en el cual debe aprender a alcanzar una meta optimizando su comportamiento mediante la definición de una función de recompensas y un conjunto de acciones posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96274a99",
   "metadata": {},
   "source": [
    "## 2. Propocito del agente\n",
    "\n",
    "El propósito del agente consiste en aprender una política óptima que maximice la recompensa acumulada al desplazarse dentro del entorno laberinto. Desde un estado inicial (punto rojo), el agente puede ejecutar acciones que modifican su posición dentro de la  cuadrícula de 8×7 . El desafío consiste en descubrir, mediante exploración y aprendizaje, la secuencia de acciones (política) que maximiza la recompensa final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e40c54",
   "metadata": {},
   "source": [
    "## 3. Objetivo\n",
    "\n",
    "El objetivo en este punto intermedio del desarrollo del proyecto es caracterizar los elementos que componen la solución del problema de navegación del agente en el entorno GridWorld. Esto implica definir los elementos del entorno de Reinforcement Learning: el conjunto de estados, las acciones posibles y la función de recompensa; \n",
    "siendo estos los que determinan el comportamiento del agente y la forma en que aprende su tarea: alcanzar la meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3b9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import Image, display, clear_output\n",
    "import matplotlib.animation as animation\n",
    "# Configurar matplotlib para mostrar gráficos en el notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4159f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self, file=None, start=(6,0), goal=(1,6)):\n",
    "        \"\"\"\n",
    "        Inicializa el laberinto.\n",
    "        Coordenadas de celdas: (fila, columna) con fila en [0,n-1], columna en [0,m-1].\n",
    "        El archivo describe segmentos de pared en coordenadas de vértices (x,y) donde\n",
    "        x=fila de vértice, y=columna de vértice, y esos rangos van 0..n y 0..m.\n",
    "        start: posición inicial dentro de la grilla.\n",
    "        goal: posición objetivo dentro de la grilla.\n",
    "        \"\"\"\n",
    "        self.n = 0  # filas\n",
    "        self.m = 0  # columnas\n",
    "        self.paredes = []  # lista de segmentos de pared [(x1, y1, x2, y2), ...]\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        if file:\n",
    "            self.load_from_file(file)\n",
    "\n",
    "    def load_from_file(self, file):\n",
    "        \"\"\"\n",
    "        Carga el laberinto desde un archivo de texto.\n",
    "        Primera línea: n m (filas columnas)\n",
    "        Segunda línea: k (cantidad de segmentos)\n",
    "        Siguientes k líneas: x1 y1 x2 y2 (segmentos entre vértices)\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            lineas = [linea.strip() for linea in f.readlines() if linea.strip()]\n",
    "        self.n, self.m = map(int, lineas[0].split())\n",
    "        k = int(lineas[1])\n",
    "        if len(lineas) < 2 + k:\n",
    "            raise ValueError(f\"El archivo debe tener al menos {2 + k} líneas, pero solo tiene {len(lineas)}\")\n",
    "        self.paredes = []\n",
    "        for i in range(2, 2 + k):\n",
    "            x1, y1, x2, y2 = map(int, lineas[i].split())\n",
    "            self.paredes.append((x1, y1, x2, y2))\n",
    "\n",
    "    def visualize(self, agente_pos=None, objetivo_pos=None, mostrar_coordenadas=True):\n",
    "        \"\"\"\n",
    "        Visualiza el laberinto usando matplotlib.\n",
    "        agente_pos: (fila, columna)\n",
    "        objetivo_pos: (fila, columna)\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        # Dibujar la cuadrícula (líneas claras)\n",
    "        for i in range(self.n + 1):\n",
    "            ax.plot([0, self.m], [i, i], 'lightgray', linewidth=0.5)\n",
    "        for j in range(self.m + 1):\n",
    "            ax.plot([j, j], [0, self.n], 'lightgray', linewidth=0.5)\n",
    "        # Dibujar paredes (segmentos entre vértices). Aquí x=fila, y=columna -> para plot usamos (columna, fila)\n",
    "        for (x1, y1, x2, y2) in self.paredes:\n",
    "            ax.plot([y1, y2], [x1, x2], 'black', linewidth=2)\n",
    "        # Dibujar agente\n",
    "        if agente_pos is not None:\n",
    "            fila, col = agente_pos\n",
    "            ax.add_patch(plt.Circle((col + 0.5, fila + 0.5), 0.25, color='green', alpha=0.7))\n",
    "            ax.text(col + 0.5, fila + 0.5, 'S', ha='center', va='center', fontsize=10, color='white', weight='bold')\n",
    "        # Dibujar objetivo\n",
    "        if objetivo_pos is not None:\n",
    "            fila, col = objetivo_pos\n",
    "            ax.add_patch(plt.Circle((col + 0.5, fila + 0.5), 0.30, color='red', alpha=0.7))\n",
    "            ax.text(col + 0.5, fila + 0.5, 'G', ha='center', va='center', fontsize=11, color='white', weight='bold')\n",
    "        # Coordenadas de vértices\n",
    "        if mostrar_coordenadas:\n",
    "            for i in range(self.n + 1):\n",
    "                for j in range(self.m + 1):\n",
    "                    ax.text(j + 0.05, i - 0.05, f'{i},{j}', ha='left', va='top', fontsize=7,\n",
    "                            color='black', bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='none', alpha=0.85))\n",
    "        ax.set_xlim(-0.5, self.m + 0.5)\n",
    "        ax.set_ylim(-0.5, self.n + 0.5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.set_title(f'Laberinto {self.n}x{self.m}')\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    def hay_pared_entre(self, estado_actual, estado_nuevo):\n",
    "        \"\"\"\n",
    "        Comprueba si existe una pared exactamente en el borde entre dos celdas adyacentes.\n",
    "        estado: (fila, columna). Pared vertical: y constante; pared horizontal: x constante.\n",
    "        \"\"\"\n",
    "        fila1, col1 = estado_actual\n",
    "        fila2, col2 = estado_nuevo\n",
    "        # Deben ser adyacentes Manhattan dist = 1\n",
    "        if abs(fila1 - fila2) + abs(col1 - col2) != 1:\n",
    "            return False\n",
    "        # Movimiento horizontal -> verificar pared vertical entre columnas\n",
    "        if fila1 == fila2:\n",
    "            col_muro = min(col1, col2) + 1  # vértice de separación vertical\n",
    "            fila_min, fila_max = fila1, fila1 + 1\n",
    "            for (x1, y1, x2, y2) in self.paredes:\n",
    "                if y1 == y2 == col_muro and {x1, x2} == {fila_min, fila_max}:\n",
    "                    return True\n",
    "            return False\n",
    "        # Movimiento vertical -> verificar pared horizontal entre filas\n",
    "        if col1 == col2:\n",
    "            fila_muro = min(fila1, fila2) + 1\n",
    "            col_min, col_max = col1, col1 + 1\n",
    "            for (x1, y1, x2, y2) in self.paredes:\n",
    "                if x1 == x2 == fila_muro and {y1, y2} == {col_min, col_max}:\n",
    "                    return True\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "    def next_state(self, action, current_state):\n",
    "        \"\"\"Calcula siguiente estado. Si choque con pared/borde retorna mismo estado.\"\"\"\n",
    "        i, j = current_state\n",
    "        if action == 'up':\n",
    "            new = (i - 1, j)\n",
    "        elif action == 'down':\n",
    "            new = (i + 1, j)\n",
    "        elif action == 'left':\n",
    "            new = (i, j - 1)\n",
    "        elif action == 'right':\n",
    "            new = (i, j + 1)\n",
    "        else:\n",
    "            return current_state  # acción inválida\n",
    "        ni, nj = new\n",
    "        if ni < 0 or ni >= self.n or nj < 0 or nj >= self.m:\n",
    "            return current_state\n",
    "        if self.hay_pared_entre(current_state, new):\n",
    "            return current_state\n",
    "        return new\n",
    "\n",
    "    def get_reward(self, current_state, action, new_state):\n",
    "        \"\"\"Recompensa: +100 si alcanza goal, -5 si no se mueve, -1 movimiento válido.\"\"\"\n",
    "        if current_state == self.goal:\n",
    "            # Ya en meta: opcionalmente 0 por acciones extra; mantenemos 0\n",
    "            return 0\n",
    "        if new_state == current_state:\n",
    "            return -5\n",
    "        if new_state == self.goal:\n",
    "            return 100\n",
    "        return -1\n",
    "\n",
    "    def show_reward(self):\n",
    "        \"\"\"Genera lista de transiciones (S, action, S', reward) dentro de la grilla sin estados fuera.\"\"\"\n",
    "        resultado = []\n",
    "        acciones = ['up','down','left','right']\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.m):\n",
    "                current_state = (i, j)\n",
    "                for accion in acciones:\n",
    "                    new_state = self.next_state(accion, current_state)\n",
    "                    r = self.get_reward(current_state, accion, new_state)\n",
    "                    resultado.append((current_state, accion, new_state, r))\n",
    "        return resultado\n",
    "\n",
    "    def show_states(self):\n",
    "        \"\"\"Lista de todos los estados (fila,columna).\"\"\"\n",
    "        return [(i,j) for i in range(self.n) for j in range(self.m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4057915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"Entorno tipo Gym que envuelve Laberinto.\n",
    "    Proporciona API: reset(), step(action), get_all_states().\n",
    "    Acciones: up, down, left, right.\n",
    "    Recompensas: +100 si llega a goal, -5 si acción no produce movimiento (bloqueo pared/borde), -1 de lo contrario.\n",
    "    \"\"\"\n",
    "    def __init__(self, maze_file: str, start_state=(6, 0), goal_state=(1, 6)):\n",
    "        self.lab = Maze(file=maze_file, start=start_state, goal=goal_state)\n",
    "        self.start_state = start_state\n",
    "        self.goal_state = goal_state\n",
    "        self.state = self.start_state\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.board = self.create_board()\n",
    "\n",
    "    def create_board(self):\n",
    "        board = [[0 for _ in range(self.lab.m)] for _ in range(self.lab.n)]\n",
    "        si, sj = self.start_state\n",
    "        gi, gj = self.goal_state\n",
    "        board[si][sj] = 1\n",
    "        board[gi][gj] = 2\n",
    "        return board\n",
    "\n",
    "    def is_valid_action(self, state, action):\n",
    "        i, j = state\n",
    "        if action == 'up':\n",
    "            cand = (i - 1, j)\n",
    "        elif action == 'down':\n",
    "            cand = (i + 1, j)\n",
    "        elif action == 'left':\n",
    "            cand = (i, j - 1)\n",
    "        elif action == 'right':\n",
    "            cand = (i, j + 1)\n",
    "        else:\n",
    "            return False\n",
    "        ni, nj = cand\n",
    "        # límites\n",
    "        if ni < 0 or ni >= self.lab.n or nj < 0 or nj >= self.lab.m:\n",
    "            return False\n",
    "        # pared\n",
    "        if self.lab.hay_pared_entre(state, cand):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        if not self.is_valid_action(state, action):\n",
    "            return state\n",
    "        i, j = state\n",
    "        if action == 'up':\n",
    "            return (i - 1, j)\n",
    "        elif action == 'down':\n",
    "            return (i + 1, j)\n",
    "        elif action == 'left':\n",
    "            return (i, j - 1)\n",
    "        elif action == 'right':\n",
    "            return (i, j + 1)\n",
    "        return state\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == self.goal_state:\n",
    "            return 100\n",
    "        if next_state == state:\n",
    "            return -5\n",
    "        return -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.get_next_state(self.state, action)\n",
    "        reward = self.get_reward(self.state, action, next_state)\n",
    "        done = (next_state == self.goal_state)\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_current_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return [(i, j) for i in range(self.lab.n) for j in range(self.lab.m)]\n",
    "\n",
    "    def visualize(self):\n",
    "        # Delegar a Laberinto pero mostrando agente actual\n",
    "        self.lab.visualize(agente_pos=self.state, objetivo_pos=self.goal_state, mostrar_coordenadas=False)\n",
    "\n",
    "    def do_action(self, action):\n",
    "        next_state = self.get_next_state(self.state, action)\n",
    "        reward = self.get_reward(self.state, action, next_state)\n",
    "        self.state = next_state\n",
    "        is_done = (next_state == self.goal_state)\n",
    "        return next_state, reward, is_done\n",
    "\n",
    "    def get_possible_actions(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.state\n",
    "        return [a for a in self.actions if self.is_valid_action(state, a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846b670",
   "metadata": {},
   "source": [
    "Definicion del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f75171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Agente que aprende usando Q-Learning en un entorno GridWorld.\n",
    "    Parámetros:\n",
    "    - env: entorno GridWorld\n",
    "    - alpha: tasa de aprendizaje\n",
    "    - gamma: factor de descuento\n",
    "    - epsilon: probabilidad de exploración\n",
    "    \"\"\"\n",
    "    def __init__(self, env: GridWorld, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.qtable = {}\n",
    "        #self.q_table = pd.DataFrame(0, index=pd.MultiIndex.from_tuples(env.get_all_states()), columns=env.actions)\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        return self.qtable.get((state, action), 0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.env.get_possible_actions(state))\n",
    "        return self.best_action(state)\n",
    "\n",
    "    def best_action(self, state):\n",
    "        state_actions = {action: self.get_value(state, action) for action in self.env.get_possible_actions(state)}\n",
    "        max_q = max(state_actions.values())\n",
    "        best_actions = [action for action, q in state_actions.items() if q == max_q]\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        next_state, reward, done = self.env.do_action(action)\n",
    "        info = \"Ejecutando acción: \" + action + \" desde el estado: \" + str(self.env.get_current_state())\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def update_values(self, state, action, reward, next_state):\n",
    "        current_q = self.get_value(state, action)\n",
    "        best_next_action = self.best_action(next_state)\n",
    "        max_next_q = self.get_value(next_state, best_next_action)\n",
    "        target = reward + self.gamma * max_next_q\n",
    "        new_q = (1- self.alpha) * current_q + self.alpha * (target - current_q)\n",
    "        self.qtable[(state, action)] = new_q\n",
    "        \n",
    "    def run(self, episodes=1000, max_steps_per_episode=100, epsilon_decay=0.999, epsilon_min=0.05):\n",
    "        # Tracking\n",
    "        episode_rewards = []\n",
    "        steps_per_episode = []\n",
    "        for ep in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "            steps = 0\n",
    "            \n",
    "            for t in range(max_steps_per_episode):\n",
    "                # Elegir acción usando el agente\n",
    "                possible_actions = self.env.get_possible_actions(state)\n",
    "                if not possible_actions:\n",
    "                    break\n",
    "                \n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, info = self.step(state, action)\n",
    "                \n",
    "                # Aprender de la experiencia\n",
    "                self.update_values(state, action, reward, next_state)\n",
    "                \n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            episode_rewards.append(total_reward)\n",
    "            steps_per_episode.append(steps)\n",
    "            \n",
    "            # Decaimiento epsilon\n",
    "            if self.epsilon > epsilon_min:\n",
    "                self.epsilon = max(epsilon_min, self.epsilon * epsilon_decay)\n",
    "            \n",
    "            # Log ocasional\n",
    "            if (ep+1) % 500 == 0:\n",
    "                avg_last = sum(episode_rewards[-100:]) / max(1, len(episode_rewards[-100:]))\n",
    "                print(f\"Episodio {ep+1} | ε={self.epsilon:.3f} | Recompensa media últimas 100: {avg_last:.2f}\")\n",
    "\n",
    "        print(\"Entrenamiento terminado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4eb3cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio: (6, 0) Meta: (1, 6)\n",
      "Posible acciones desde inicio: ['up']\n",
      "\n",
      "Board (0 libre, 1 inicio, 2 meta):\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear el entorno con la nueva clase GridWorld\n",
    "env = GridWorld('laberintos/project_lab_v2.txt')\n",
    "print('Inicio:', env.start_state, 'Meta:', env.goal_state)\n",
    "print('Posible acciones desde inicio:', env.get_possible_actions())\n",
    "print('\\nBoard (0 libre, 1 inicio, 2 meta):')\n",
    "print(np.array(env.board))\n",
    "\n",
    "# Visualizar\n",
    "env.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4c134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "EPISODES = 3000          # Ajusta según convergencia\n",
    "ALPHA = 0.15             # Tasa de aprendizaje\n",
    "GAMMA = 0.95             # Factor de descuento\n",
    "EPSILON_START = 0.9      # Exploración inicial\n",
    "EPSILON_MIN = 0.05       # Exploración mínima\n",
    "EPSILON_DECAY = 0.999    # Decaimiento multiplicativo por episodio\n",
    "MAX_STEPS_EPISODE = 500  # Tope para evitar loops infinitos\n",
    "\n",
    "# Inicializar entorno y agente\n",
    "env = GridWorld('laberintos/project_lab_v2.txt')\n",
    "agent = QLearningAgent(env, alpha=ALPHA, gamma=GAMMA, epsilon=EPSILON_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e02a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 500 | ε=0.546 | Recompensa media últimas 100: 40.86\n",
      "Episodio 1000 | ε=0.331 | Recompensa media últimas 100: 61.32\n",
      "Episodio 1500 | ε=0.201 | Recompensa media últimas 100: 69.32\n",
      "Episodio 2000 | ε=0.122 | Recompensa media últimas 100: 72.14\n",
      "Episodio 2500 | ε=0.074 | Recompensa media últimas 100: 73.40\n",
      "Episodio 3000 | ε=0.050 | Recompensa media últimas 100: 74.82\n",
      "Entrenamiento terminado.\n"
     ]
    }
   ],
   "source": [
    "agent.run(episodes=EPISODES, max_steps_per_episode=MAX_STEPS_EPISODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7505143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
